{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/sms-spam-data.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data[[\"v1\", \"v2\"]].rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\"label\"], drop_first=True).rename(columns={\"label_spam\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    4825\n",
       "1     747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    return len(string.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_count_ratio(df):\n",
    "    n = df.shape[0]\n",
    "    word_counts = []\n",
    "    for row in df[\"text\"]:\n",
    "        word_counts.append(count_words(row))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)\n",
    "    return n/avg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357.0242292521935"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_count_ratio(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://developers.google.com/machine-learning/guides/text-classification/step-2-5 this indicates that we should go with an n-gram preprocessing step couple with a simple MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to experimenting with an n-gram based model. The pre-processing steps are as follows:\n",
    "- Tokenize text-sample into 1 and 2 word n-grams. This mean extracing each individual word as well as each pair of consecutive words. The combination of both steps\n",
    "- Vectorize the samples using a TF-IDF encoding scheme. Each piece of text is converted into a vector capturing which n-grams are present in it.\n",
    "- Drop the least common n-gram tokens by discarding those that occur fewer than two times and and using statistical tests to determine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram sizes to compute\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Limit on the number of features\n",
    "TOP_K = 10000\n",
    "# Whether text should be split into word or character n-grams\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def ngram_vectorize(train_text: List[str], train_labels: np.ndarray, test_text: List[str]):\n",
    "    # Arguments for vectorizor\n",
    "    kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    \n",
    "    # Vectorize training text\n",
    "    x_train = vectorizer.fit_transform(train_text)\n",
    "    # Vectorize test text\n",
    "    x_test = vectorizer.transform(test_text)\n",
    "\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(f_classif, k=TOP_K)\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare the data. First we create a train and test split and then wrangle the results into the appropriate format for our `ngram_vectorize` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(x_train)\n",
    "y_train = y_train.to_numpy()\n",
    "x_test = list(x_test)\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerardo/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1795: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = ngram_vectorize(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def build_model(layers: int, units: int, dropout_rate: float, input_shape: Tuple):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units, input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,\n",
    "                train_labels,\n",
    "                test_data,\n",
    "                test_labels,\n",
    "                learning_rate=1e-3,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                layers=2,\n",
    "                units=64,\n",
    "                dropout_rate=0.2):\n",
    "    # Create model\n",
    "    model = build_model(layers=layers,units=units, dropout_rate=dropout_rate, input_shape=x_train.shape[1:])\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    loss = 'binary_crossentropy'\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc', tf.keras.metrics.FalseNegatives(name=\"fn\")])\n",
    "\n",
    "    # Create early stopping callback\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(train_data, \n",
    "              train_labels, \n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(test_data, test_labels),\n",
    "              verbose=2,\n",
    "              batch_size=batch_size)\n",
    "\n",
    "    # Print results\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}, false negatives: {fn}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1], fn=history['fn'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerardo/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "/home/gerardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "140/140 - 1s - loss: 0.2740 - acc: 0.9100 - fn: 382.0000 - val_loss: 0.0614 - val_acc: 0.9839 - val_fn: 17.0000\n",
      "Epoch 2/100\n",
      "140/140 - 1s - loss: 0.0318 - acc: 0.9917 - fn: 29.0000 - val_loss: 0.0369 - val_acc: 0.9892 - val_fn: 7.0000\n",
      "Epoch 3/100\n",
      "140/140 - 1s - loss: 0.0126 - acc: 0.9964 - fn: 11.0000 - val_loss: 0.0380 - val_acc: 0.9892 - val_fn: 10.0000\n",
      "Epoch 4/100\n",
      "140/140 - 1s - loss: 0.0048 - acc: 0.9984 - fn: 5.0000 - val_loss: 0.0402 - val_acc: 0.9901 - val_fn: 10.0000\n",
      "Epoch 5/100\n",
      "140/140 - 1s - loss: 0.0026 - acc: 0.9996 - fn: 2.0000 - val_loss: 0.0418 - val_acc: 0.9901 - val_fn: 10.0000\n",
      "Validation accuracy: 0.9901345372200012, loss: 0.04184374958276749, false negatives: 2.0\n"
     ]
    }
   ],
   "source": [
    "train_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems to be doing remarkably well, only misclassifying 2 spam messages"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
